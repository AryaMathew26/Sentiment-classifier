{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment Two:  Sentiment Classification\n",
    "\n",
    "For this exercise you will be using the \"SemEval 2017 task 4\" corpus provided on the module website, available through the following link: https://warwick.ac.uk/fac/sci/dcs/teaching/material/cs918/semeval-tweets.tar.bz2 You will focus particularly on Subtask A, i.e. classifying the overall sentiment of a tweet as positive, negative or neutral.\n",
    "\n",
    "You are requested to produce a Jupyter notebook for the coursework submission. The input to your program is the SemEval data downloaded. Note that TAs need to run your program on their own machine by using the original SemEval data. As such, don’t submit a Python program that takes as input some preprocessed files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import necessary packages\n",
    "You may import more packages here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary packages\n",
    "import re\n",
    "from os.path import join\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My Pytorch version: 2.0.0\n",
      "Torch Text Version : 0.15.1\n"
     ]
    }
   ],
   "source": [
    "# Import packages\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "import torch\n",
    "print(\"My Pytorch version: \" + torch.__version__)\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "# Set the device to perform the computation\n",
    "DEVICE = torch.device('mps')\n",
    "\n",
    "# Set a fixed seed for reproducibility\n",
    "def setup_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    \n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "import torchtext\n",
    "print(\"Torch Text Version : {}\".format(torchtext.__version__))\n",
    "\n",
    "import json\n",
    "import math\n",
    "import nltk\n",
    "import os, sys\n",
    "from nltk.util import ngrams\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from torchtext.data import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "from torch.optim import Adam\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define test sets\n",
    "testsets = ['twitter-test1.txt', 'twitter-test2.txt', 'twitter-test3.txt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skeleton: Evaluation code for the test sets\n",
    "def read_test(testset):\n",
    "    '''\n",
    "    readin the testset and return a dictionary\n",
    "    :param testset: str, the file name of the testset to compare\n",
    "    '''\n",
    "    id_gts = {}\n",
    "    with open(testset, 'r', encoding='utf8') as fh:\n",
    "        for line in fh:\n",
    "            fields = line.split('\\t')\n",
    "            tweetid = fields[0]\n",
    "            gt = fields[1]\n",
    "\n",
    "            id_gts[tweetid] = gt\n",
    "\n",
    "    return id_gts\n",
    "\n",
    "\n",
    "def confusion(id_preds, testset, classifier):\n",
    "    '''\n",
    "    print the confusion matrix of {'positive', 'netative'} between preds and testset\n",
    "    :param id_preds: a dictionary of predictions formated as {<tweetid>:<sentiment>, ... }\n",
    "    :param testset: str, the file name of the testset to compare\n",
    "    :classifier: str, the name of the classifier\n",
    "    '''\n",
    "    id_gts = read_test(testset)\n",
    "\n",
    "    gts = []\n",
    "    for m, c1 in id_gts.items():\n",
    "        if c1 not in gts:\n",
    "            gts.append(c1)\n",
    "\n",
    "    gts = ['positive', 'negative', 'neutral']\n",
    "\n",
    "    conf = {}\n",
    "    for c1 in gts:\n",
    "        conf[c1] = {}\n",
    "        for c2 in gts:\n",
    "            conf[c1][c2] = 0\n",
    "\n",
    "    for tweetid, gt in id_gts.items():\n",
    "        if tweetid in id_preds:\n",
    "            pred = id_preds[tweetid]\n",
    "        else:\n",
    "            pred = 'neutral'\n",
    "        conf[pred][gt] += 1\n",
    "\n",
    "    print(''.ljust(12) + '  '.join(gts))\n",
    "\n",
    "    for c1 in gts:\n",
    "        print(c1.ljust(12), end='')\n",
    "        for c2 in gts:\n",
    "            if sum(conf[c1].values()) > 0:\n",
    "                print('%.3f     ' % (conf[c1][c2] / float(sum(conf[c1].values()))), end='')\n",
    "            else:\n",
    "                print('0.000     ', end='')\n",
    "        print('')\n",
    "\n",
    "    print('')\n",
    "\n",
    "\n",
    "def evaluate(id_preds, testset, classifier):\n",
    "    '''\n",
    "    print the macro-F1 score of {'positive', 'netative'} between preds and testset\n",
    "    :param id_preds: a dictionary of predictions formated as {<tweetid>:<sentiment>, ... }\n",
    "    :param testset: str, the file name of the testset to compare\n",
    "    :classifier: str, the name of the classifier\n",
    "    '''\n",
    "    id_gts = read_test(testset)\n",
    "\n",
    "    acc_by_class = {}\n",
    "    for gt in ['positive', 'negative', 'neutral']:\n",
    "        acc_by_class[gt] = {'tp': 0, 'fp': 0, 'tn': 0, 'fn': 0}\n",
    "\n",
    "    catf1s = {}\n",
    "\n",
    "    ok = 0\n",
    "    for tweetid, gt in id_gts.items():\n",
    "        if tweetid in id_preds:\n",
    "            pred = id_preds[tweetid]\n",
    "        else:\n",
    "            pred = 'neutral'\n",
    "\n",
    "        if gt == pred:\n",
    "            ok += 1\n",
    "            acc_by_class[gt]['tp'] += 1\n",
    "        else:\n",
    "            acc_by_class[gt]['fn'] += 1\n",
    "            acc_by_class[pred]['fp'] += 1\n",
    "\n",
    "    catcount = 0\n",
    "    itemcount = 0\n",
    "    macro = {'p': 0, 'r': 0, 'f1': 0}\n",
    "    micro = {'p': 0, 'r': 0, 'f1': 0}\n",
    "    semevalmacro = {'p': 0, 'r': 0, 'f1': 0}\n",
    "\n",
    "    microtp = 0\n",
    "    microfp = 0\n",
    "    microtn = 0\n",
    "    microfn = 0\n",
    "    for cat, acc in acc_by_class.items():\n",
    "        catcount += 1\n",
    "\n",
    "        microtp += acc['tp']\n",
    "        microfp += acc['fp']\n",
    "        microtn += acc['tn']\n",
    "        microfn += acc['fn']\n",
    "\n",
    "        p = 0\n",
    "        if (acc['tp'] + acc['fp']) > 0:\n",
    "            p = float(acc['tp']) / (acc['tp'] + acc['fp'])\n",
    "\n",
    "        r = 0\n",
    "        if (acc['tp'] + acc['fn']) > 0:\n",
    "            r = float(acc['tp']) / (acc['tp'] + acc['fn'])\n",
    "\n",
    "        f1 = 0\n",
    "        if (p + r) > 0:\n",
    "            f1 = 2 * p * r / (p + r)\n",
    "\n",
    "        catf1s[cat] = f1\n",
    "\n",
    "        n = acc['tp'] + acc['fn']\n",
    "\n",
    "        macro['p'] += p\n",
    "        macro['r'] += r\n",
    "        macro['f1'] += f1\n",
    "\n",
    "        if cat in ['positive', 'negative']:\n",
    "            semevalmacro['p'] += p\n",
    "            semevalmacro['r'] += r\n",
    "            semevalmacro['f1'] += f1\n",
    "\n",
    "        itemcount += n\n",
    "\n",
    "    micro['p'] = float(microtp) / float(microtp + microfp)\n",
    "    micro['r'] = float(microtp) / float(microtp + microfn)\n",
    "    micro['f1'] = 2 * float(micro['p']) * micro['r'] / float(micro['p'] + micro['r'])\n",
    "\n",
    "    semevalmacrof1 = semevalmacro['f1'] / 2\n",
    "\n",
    "    print(testset + ' (' + classifier + '): %.3f' % semevalmacrof1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load training set, dev set and testing set\n",
    "Here, you need to load the training set, the development set and the test set. For better classification results, you may need to preprocess tweets before sending them to the classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training set, dev set and testing set\n",
    "data = {}\n",
    "tweetids = {}\n",
    "tweetgts = {}\n",
    "tweetgts_num = {}\n",
    "tweets = {}\n",
    "\n",
    "# not given\n",
    "# may need to substitute using tweets\n",
    "tweets_preprocessed = {}\n",
    "\n",
    "for dataset in ['twitter-training-data.txt','twitter-dev-data.txt'] + testsets:\n",
    "    data[dataset] = []\n",
    "    tweets[dataset] = []\n",
    "    tweetids[dataset] = []\n",
    "    tweetgts[dataset] = []\n",
    "    tweetgts_num[dataset] = []\n",
    "    tweets_preprocessed[dataset] = []\n",
    "\n",
    "    # write code to read in the datasets here\n",
    "    with open(dataset) as f:\n",
    "        for line in f:\n",
    "            twitter_object = line # load the line, containing a twitter object\n",
    "            twitter_object = twitter_object.lower() # convert to lowercase\n",
    "            words_list = twitter_object.split()\n",
    "            words_req = words_list[2:]\n",
    "            tweet = ' '.join(words_req)\n",
    "            ids_reqd=words_list[0]\n",
    "            sentiment = words_list[1]\n",
    "            if sentiment == 'positive' or sentiment =='neutral':\n",
    "                sent_int = 0\n",
    "            else:\n",
    "                sent_int = 1\n",
    "            # list of twitter objects\n",
    "            data[dataset].append(twitter_object)\n",
    "            # list of tweets\n",
    "            tweets[dataset].append(tweet)\n",
    "            # list of ids\n",
    "            tweetids[dataset].append(ids_reqd)\n",
    "            # list of sentiments\n",
    "            tweetgts[dataset].append(sentiment)\n",
    "            # list of sentiments num\n",
    "            tweetgts_num[dataset].append(sent_int)\n",
    "            \n",
    "    # Tweet preprocessing\n",
    "    \n",
    "    # Apply our RegExp to the example document\n",
    "    # Remove twitter handles\n",
    "    new_tweets = []\n",
    "    for tweet in tweets[dataset]:\n",
    "        tweet_without_handles = re.sub('@[\\w]*', '', tweet)\n",
    "        new_tweets.append(tweet_without_handles)\n",
    "\n",
    "    # Remove URLs\n",
    "    new_tweets1 = []\n",
    "    for tweet in new_tweets:\n",
    "        tweet_without_url = re.sub(r\"((https?|ftp)://)?[a-z0-9\\-._~:/?#\\[\\]@!$&'()*+,;=%]+\\.[a-z]{2,}[a-z0-9\\-._~:/?#[\\]@!$&'()*+,;=%]*\",\"\",tweet)\n",
    "        new_tweets1.append(tweet_without_url)\n",
    "    \n",
    "    # Remove non alpha characters except hashtags\n",
    "    new_tweets_2=[]\n",
    "    for tweet in new_tweets1:\n",
    "        tweet_without_nonalpha = re.sub(r\"[^A-Za-z# ]\",\"\",tweet)\n",
    "        new_tweets_2.append(tweet_without_nonalpha)\n",
    "    \n",
    "    # Remove numbers\n",
    "    new_tweets_3=[]\n",
    "    for tweet in new_tweets_2:\n",
    "        tweets_without_num = re.sub(r\"\\b[0-9]+\\b\",\"\",tweet)\n",
    "        new_tweets_3.append(tweets_without_num)\n",
    "    \n",
    "    # Remove words with three or less characters\n",
    "    for tweet in new_tweets_3:\n",
    "        tweet_with_atleast_four_char = ' '.join(word for word in tweet.split() if len(word)>3)\n",
    "        tweets_preprocessed[dataset].append(tweet_with_atleast_four_char)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check how to handle hashtags."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preliminary Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualise the preprocessed training set in a dataframe as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      ids                                             tweets  \\\n",
      "0      335104872099066692  felt privileged play fighters songs guitar tod...   \n",
      "1      796528524030124618  pakistan islamic country true muslims india lo...   \n",
      "2      760964834217238632  happy birthday coolest golfer bali become cool...   \n",
      "3      147713180324524046                       tmills going tucson thursday   \n",
      "4      732302280474120023  hmmmmm where #blacklivesmatter when matters li...   \n",
      "...                   ...                                                ...   \n",
      "45096  660374218263817235  sunday cinema paul mccartney david gilmour pau...   \n",
      "45097  739323365061217061  independence sacrifices muslims victory pakist...   \n",
      "45098  681369726697754114  september arrived which means apples iphone ju...   \n",
      "45099  922217029064536808  tomorrow some filled feeding league prolly som...   \n",
      "45100  606913141028836185  alright whos choosing paul mccartney over week...   \n",
      "\n",
      "      sentiments  \n",
      "0       positive  \n",
      "1       positive  \n",
      "2       positive  \n",
      "3       negative  \n",
      "4       negative  \n",
      "...          ...  \n",
      "45096    neutral  \n",
      "45097    neutral  \n",
      "45098   positive  \n",
      "45099   positive  \n",
      "45100    neutral  \n",
      "\n",
      "[45101 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "train_col = [tweetids['twitter-training-data.txt'], tweets_preprocessed['twitter-training-data.txt'], tweetgts['twitter-training-data.txt']]\n",
    "train_df = pd.DataFrame (train_col).transpose()\n",
    "train_df.columns = ['ids', 'tweets','sentiments']\n",
    "print(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 45101 entries, 0 to 45100\n",
      "Data columns (total 3 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   ids         45101 non-null  object\n",
      " 1   tweets      45101 non-null  object\n",
      " 2   sentiments  45101 non-null  object\n",
      "dtypes: object(3)\n",
      "memory usage: 1.0+ MB\n"
     ]
    }
   ],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method IndexOpsMixin.value_counts of 0        felt privileged play fighters songs guitar tod...\n",
       "1        pakistan islamic country true muslims india lo...\n",
       "2        happy birthday coolest golfer bali become cool...\n",
       "3                             tmills going tucson thursday\n",
       "4        hmmmmm where #blacklivesmatter when matters li...\n",
       "                               ...                        \n",
       "45096    sunday cinema paul mccartney david gilmour pau...\n",
       "45097    independence sacrifices muslims victory pakist...\n",
       "45098    september arrived which means apples iphone ju...\n",
       "45099    tomorrow some filled feeding league prolly som...\n",
       "45100    alright whos choosing paul mccartney over week...\n",
       "Name: tweets, Length: 45101, dtype: object>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['tweets'].value_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "neutral     20789\n",
       "positive    15986\n",
       "negative     8326\n",
       "Name: sentiments, dtype: int64"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check for class imbalance\n",
    "train_df['sentiments'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that there are very small number of negative tweets compared to neutral and positive. We need to accurately predict negative classes. So we will group the neutral and positive classes together as 0, and the negative labels will be encoded as 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [felt, privileged, play, fighters, songs, guit...\n",
       "1    [pakistan, islamic, country, true, muslims, in...\n",
       "2    [happy, birthday, coolest, golfer, bali, becom...\n",
       "3                    [tmills, going, tucson, thursday]\n",
       "4    [hmmmmm, where, #blacklivesmatter, when, matte...\n",
       "Name: tweets, dtype: object"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenisation\n",
    "tokenized_tweet = train_df['tweets'].apply(lambda x: x.split())\n",
    "tokenized_tweet.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [felt, privileg, play, fighter, song, guitar, ...\n",
       "1    [pakistan, islam, countri, true, muslim, india...\n",
       "2    [happi, birthday, coolest, golfer, bali, becom...\n",
       "3                        [tmill, go, tucson, thursday]\n",
       "4    [hmmmmm, where, #blacklivesmatt, when, matter,...\n",
       "Name: tweets, dtype: object"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# stemming\n",
    "from nltk.stem.porter import *\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "tokenized_tweet = tokenized_tweet.apply(lambda x: [stemmer.stem(i) for i in x]) # stemming\n",
    "tokenized_tweet.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ids</th>\n",
       "      <th>tweets</th>\n",
       "      <th>sentiments</th>\n",
       "      <th>tidy_tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>335104872099066692</td>\n",
       "      <td>felt privileged play fighters songs guitar tod...</td>\n",
       "      <td>positive</td>\n",
       "      <td>felt privileg play fighter song guitar today w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>796528524030124618</td>\n",
       "      <td>pakistan islamic country true muslims india lo...</td>\n",
       "      <td>positive</td>\n",
       "      <td>pakistan islam countri true muslim india love ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>760964834217238632</td>\n",
       "      <td>happy birthday coolest golfer bali become cool...</td>\n",
       "      <td>positive</td>\n",
       "      <td>happi birthday coolest golfer bali becom coole...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>147713180324524046</td>\n",
       "      <td>tmills going tucson thursday</td>\n",
       "      <td>negative</td>\n",
       "      <td>tmill go tucson thursday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>732302280474120023</td>\n",
       "      <td>hmmmmm where #blacklivesmatter when matters li...</td>\n",
       "      <td>negative</td>\n",
       "      <td>hmmmmm where #blacklivesmatt when matter like ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45096</th>\n",
       "      <td>660374218263817235</td>\n",
       "      <td>sunday cinema paul mccartney david gilmour pau...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>sunday cinema paul mccartney david gilmour pau...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45097</th>\n",
       "      <td>739323365061217061</td>\n",
       "      <td>independence sacrifices muslims victory pakist...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>independ sacrific muslim victori pakistan proud</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45098</th>\n",
       "      <td>681369726697754114</td>\n",
       "      <td>september arrived which means apples iphone ju...</td>\n",
       "      <td>positive</td>\n",
       "      <td>septemb arriv which mean appl iphon just hour ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45099</th>\n",
       "      <td>922217029064536808</td>\n",
       "      <td>tomorrow some filled feeding league prolly som...</td>\n",
       "      <td>positive</td>\n",
       "      <td>tomorrow some fill feed leagu prolli some skul...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45100</th>\n",
       "      <td>606913141028836185</td>\n",
       "      <td>alright whos choosing paul mccartney over week...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>alright who choos paul mccartney over weeknd t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>45101 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      ids                                             tweets  \\\n",
       "0      335104872099066692  felt privileged play fighters songs guitar tod...   \n",
       "1      796528524030124618  pakistan islamic country true muslims india lo...   \n",
       "2      760964834217238632  happy birthday coolest golfer bali become cool...   \n",
       "3      147713180324524046                       tmills going tucson thursday   \n",
       "4      732302280474120023  hmmmmm where #blacklivesmatter when matters li...   \n",
       "...                   ...                                                ...   \n",
       "45096  660374218263817235  sunday cinema paul mccartney david gilmour pau...   \n",
       "45097  739323365061217061  independence sacrifices muslims victory pakist...   \n",
       "45098  681369726697754114  september arrived which means apples iphone ju...   \n",
       "45099  922217029064536808  tomorrow some filled feeding league prolly som...   \n",
       "45100  606913141028836185  alright whos choosing paul mccartney over week...   \n",
       "\n",
       "      sentiments                                         tidy_tweet  \n",
       "0       positive  felt privileg play fighter song guitar today w...  \n",
       "1       positive  pakistan islam countri true muslim india love ...  \n",
       "2       positive  happi birthday coolest golfer bali becom coole...  \n",
       "3       negative                           tmill go tucson thursday  \n",
       "4       negative  hmmmmm where #blacklivesmatt when matter like ...  \n",
       "...          ...                                                ...  \n",
       "45096    neutral  sunday cinema paul mccartney david gilmour pau...  \n",
       "45097    neutral    independ sacrific muslim victori pakistan proud  \n",
       "45098   positive  septemb arriv which mean appl iphon just hour ...  \n",
       "45099   positive  tomorrow some fill feed leagu prolli some skul...  \n",
       "45100    neutral  alright who choos paul mccartney over weeknd t...  \n",
       "\n",
       "[45101 rows x 4 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# stitch these tokens back together\n",
    "for i in range(len(tokenized_tweet)):\n",
    "    tokenized_tweet[i] = ' '.join(tokenized_tweet[i])\n",
    "\n",
    "train_df['tidy_tweet'] = tokenized_tweet\n",
    "train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemming is not required as it creates some meaningless tokens and changes the meaning for some other tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Traditional ML models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "twitter-dev-data.txt (Logistic Regression): 0.159\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aryamathew/opt/anaconda3/envs/tensorflow/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "# training + validation\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "classifier = 'Logistic Regression'\n",
    "testset = 'twitter-dev-data.txt'\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.90, min_df=2, max_features=1000, stop_words='english')\n",
    "\n",
    "document = []\n",
    "for dataset in ['twitter-training-data.txt', testset] :\n",
    "    document += tweets_preprocessed[dataset]\n",
    "\n",
    "# TF-IDF feature matrix\n",
    "tfidf = tfidf_vectorizer.fit_transform(document)\n",
    "\n",
    "# labels\n",
    "ytrain = tweetgts['twitter-training-data.txt']\n",
    "ytest = tweetgts[testset]\n",
    "\n",
    "train_tfidf = tfidf[:45101,:]\n",
    "test_tfidf = tfidf[45101:,:]\n",
    "\n",
    "xtrain_tfidf = train_tfidf[range(len(ytrain))]\n",
    "xtest_tfidf = test_tfidf[range(len(ytest))]\n",
    "\n",
    "if classifier == 'Logistic Regression':\n",
    "    lreg = LogisticRegression()\n",
    "    lreg.fit(xtrain_tfidf, ytrain)\n",
    "\n",
    "    preds = lreg.predict(xtest_tfidf)\n",
    "    pred_list = list(preds)\n",
    "    \n",
    "elif classifier == 'SVM':\n",
    "    svc = svm.SVC(kernel='linear', C=1).fit(xtrain_tfidf, ytrain)\n",
    "\n",
    "    preds = svc.predict(xtest_tfidf)\n",
    "    pred_list = list(preds)\n",
    "    \n",
    "elif classifier == 'Random Forest':\n",
    "    rf = RandomForestClassifier(n_estimators=400, random_state=11).fit(xtrain_tfidf, ytrain)\n",
    "    preds = rf.predict(xtest_tfidf)\n",
    "    pred_list = list(preds)\n",
    "    \n",
    "elif classifier == 'Decision Tree':\n",
    "    model_dec = DecisionTreeClassifier(max_depth=10, random_state=0)\n",
    "    model_dec.fit(xtrain_tfidf, y_train)\n",
    "    preds = model_dec.predict(xtest_tfidf)\n",
    "    pred_list = list(preds)\n",
    "    \n",
    "for i in range(len(pred_list)):\n",
    "    if pred_list[i]==0:\n",
    "        pred_list[i]='positive'\n",
    "    else:\n",
    "        pred_list[i]='negative'\n",
    "        \n",
    "id_preds = dict(zip(tweetids[testset], pred_list))\n",
    "\n",
    "evaluate(id_preds, testset, classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use gridsearch to perform hyperparameter optimisation\n",
    "# from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# parameters = {'kernel': ('linear', 'rbf'), 'C': (0.1,1,10,100,1000)}\n",
    "\n",
    "# svc = svm.SVC()\n",
    "# clf = GridSearchCV(svc, parameters, cv=10, n_jobs=-1) ## `-1` run in parallel\n",
    "# clf.fit(xtrain_tfidf, ytrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BOW\n",
    "# Use the torchtext tokenizer, it builds a vocabulary on the training and test set\n",
    "from torchtext.data import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "\n",
    "def build_vocab(datasets):\n",
    "    for dataset in datasets:\n",
    "        for text in dataset:\n",
    "            yield tokenizer(text)\n",
    "# in tokens: the words can repeat.\n",
    "# in the dictionary, there are unique words\n",
    "# vocab: unique list of words we are going to use\n",
    "# build the dictionary\n",
    "# specials token: for all the words that are not in the dictionary\n",
    "vocab = build_vocab_from_iterator(build_vocab([tweets_preprocessed['twitter-training-data.txt'], tweets_preprocessed['twitter-dev-data.txt']]), specials=[\"<UNK>\"])\n",
    "\n",
    "# Defaults symbol for unknown words \n",
    "vocab.set_default_index(vocab[\"<UNK>\"])\n",
    "\n",
    "#creating vocabulary\n",
    "voc = [w for w in vocab.get_stoi()]\n",
    "vocab2index = {\"\":0, \"UNK\":1}\n",
    "words = [\"\", \"UNK\"]\n",
    "for word in voc:\n",
    "    vocab2index[word] = len(words)\n",
    "    words.append(word)\n",
    "    \n",
    "def encode_sentence(text, vocab2index, N=30):\n",
    "    tokenized = tokenizer(text)\n",
    "    encoded = np.zeros(N, dtype=int)\n",
    "    enc1 = np.array([vocab2index.get(word, vocab2index[\"UNK\"]) for word in tokenized])\n",
    "    length = min(N, len(enc1))\n",
    "    encoded[:length] = enc1[:length]\n",
    "    return encoded, length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode testsets 1,2,3,4,5\n",
    "tweets_encoded = {}\n",
    "for dataset in ['twitter-training-data.txt','twitter-dev-data.txt']+testsets:\n",
    "    tweets_encoded[dataset] = []\n",
    "    for preprocessed_tweet in tweets_preprocessed[dataset]:\n",
    "        encoded_tweet = encode_sentence(preprocessed_tweet, vocab2index, N=30)\n",
    "        tweets_encoded[dataset].append(encoded_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create train and val sets\n",
    "X_train = tweets_encoded['twitter-training-data.txt']\n",
    "X_valid = tweets_encoded['twitter-dev-data.txt']\n",
    "y_train = tweetgts_num['twitter-training-data.txt']\n",
    "y_valid = tweetgts_num['twitter-dev-data.txt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a dictionary of testset sentiments\n",
    "id_gts = {}\n",
    "for testset in testsets:\n",
    "    id_gts[testset] = read_test(testset)\n",
    "    for id in id_gts[testset]:\n",
    "        if id_gts[testset][id] == 'positive' or id_gts[testset][id]=='neutral':\n",
    "            id_gts[testset][id] = 0\n",
    "        else:\n",
    "            id_gts[testset][id] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create test sets\n",
    "X_test_1 = tweets_encoded['twitter-test1.txt']\n",
    "X_test_2 = tweets_encoded['twitter-test2.txt']\n",
    "X_test_3 = tweets_encoded['twitter-test3.txt']\n",
    "y_test_1 = list(id_gts['twitter-test1.txt'].values())\n",
    "y_test_2 = list(id_gts['twitter-test2.txt'].values())\n",
    "y_test_3 = list(id_gts['twitter-test3.txt'].values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BOW\n",
    "# Use the torchtext tokenizer, it builds a vocabulary on the training and test set\n",
    "from torchtext.data import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "\n",
    "def build_vocab(datasets):\n",
    "    for dataset in datasets:\n",
    "        for text in dataset:\n",
    "            yield tokenizer(text)\n",
    "# in tokens: the words can repeat.\n",
    "# in the dictionary, there are unique words\n",
    "# vocab: unique list of words we are going to use\n",
    "# build the dictionary\n",
    "# specials token: for all the words that are not in the dictionary\n",
    "vocab = build_vocab_from_iterator(build_vocab([tweets_preprocessed['twitter-training-data.txt'], tweets_preprocessed['twitter-dev-data.txt']]), specials=[\"<UNK>\"])\n",
    "\n",
    "# Defaults symbol for unknown words \n",
    "vocab.set_default_index(vocab[\"<UNK>\"])\n",
    "\n",
    "#creating vocabulary\n",
    "voc = [w for w in vocab.get_stoi()]\n",
    "vocab2index = {\"\":0, \"UNK\":1}\n",
    "words = [\"\", \"UNK\"]\n",
    "for word in voc:\n",
    "    vocab2index[word] = len(words)\n",
    "    words.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Pytorch dataset\n",
    "from torch.utils.data import Dataset\n",
    "class ReviewsDataset(Dataset):\n",
    "    def __init__(self, X, Y):\n",
    "        self.X = X\n",
    "        self.y = Y\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return torch.from_numpy(self.X[idx][0].astype(np.int32)), self.y[idx]\n",
    "    \n",
    "train_ds = ReviewsDataset(X_train, y_train)\n",
    "valid_ds = ReviewsDataset(X_valid, y_valid)\n",
    "\n",
    "batch_size = 2000\n",
    "vocab_size = len(words)\n",
    "train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "val_dl = DataLoader(valid_ds, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2000\n",
    "vocab_size = len(words)\n",
    "X = {}\n",
    "Y = {}\n",
    "ds = {}\n",
    "dl = {}\n",
    "for dataset in ['twitter-training-data.txt','twitter-dev-data.txt']+testsets:\n",
    "    X[dataset] = tweets_encoded[dataset]\n",
    "    Y[dataset] = tweetgts_num[dataset]\n",
    "    ds[dataset] = ReviewsDataset(X[dataset], Y[dataset])\n",
    "    dl[dataset] = DataLoader(ds[dataset], batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2000, 30]) torch.Size([2000])\n"
     ]
    }
   ],
   "source": [
    "# Example - 1 batch\n",
    "for X, Y in dl['twitter-dev-data.txt']:\n",
    "    print(X.shape, Y.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GLOVE\n",
    "def load_glove_vectors(glove_file=\"glove.6B.100d.txt\"):\n",
    "    \"\"\"Load the glove word vectors\"\"\"\n",
    "    word_vectors = {}\n",
    "    with open(glove_file) as f:\n",
    "        for line in f:\n",
    "            split = line.split()\n",
    "            word_vectors[split[0]] = np.array([float(x) for x in split[1:]])\n",
    "    return word_vectors\n",
    "\n",
    "def get_emb_matrix(pretrained, word_counts, emb_size = 100):\n",
    "    \"\"\" Creates embedding matrix from word vectors\"\"\"\n",
    "    vocab_size = len(word_counts) + 2\n",
    "    vocab_to_idx = {}\n",
    "    vocab = [\"\", \"UNK\"]\n",
    "    W = np.zeros((vocab_size, emb_size), dtype=\"float32\")\n",
    "    W[0] = np.zeros(emb_size, dtype='float32') # adding a vector for padding\n",
    "    W[1] = np.random.uniform(-0.25, 0.25, emb_size) # adding a vector for unknown words \n",
    "    vocab_to_idx[\"UNK\"] = 1\n",
    "    i = 2\n",
    "    for word in word_counts:\n",
    "        if word in word_vecs:\n",
    "            W[i] = word_vecs[word]\n",
    "        else:\n",
    "            W[i] = np.random.uniform(-0.25,0.25, emb_size)\n",
    "        vocab_to_idx[word] = i\n",
    "        vocab.append(word)\n",
    "        i += 1   \n",
    "    return W, np.array(vocab), vocab_to_idx\n",
    "\n",
    "voc = [w for w in vocab.get_stoi()]\n",
    "word_vecs = load_glove_vectors()\n",
    "pretrained_weights, vocab, vocab2index = get_emb_matrix(word_vecs, voc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TrainingLoop(model, loss_fn, optimizer, train_loader, val_loader, epochs=10):\n",
    "    for i in range(1, epochs+1):\n",
    "        losses = []\n",
    "        # Cycle over the training examples (using minibatches)\n",
    "        # X are the examples, Y are the associated labels\n",
    "        for X, Y in tqdm(train_loader):\n",
    "            # Make the prediction\n",
    "            Y_preds = model(X)\n",
    "\n",
    "            # Compute the loss\n",
    "            loss = loss_fn(Y_preds, Y)\n",
    "            losses.append(loss.item())\n",
    "\n",
    "            # Reset the gradient\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Compute the gradient\n",
    "            loss.backward()\n",
    "            \n",
    "            # Update the weights\n",
    "            optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    " \n",
    "learning_rate = 1e-3\n",
    "\n",
    "# Loss Function\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# Optimizer\n",
    "#optimizer = optim.SGD(text_classifier.parameters(), lr=0.01, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Prediction(model, loss_fn, dataset):\n",
    "    # -- Disable the gradient --\n",
    "    # saves computations\n",
    "    with torch.no_grad():\n",
    "        Y_shuffled, Y_preds, losses = [],[],[]\n",
    "        for X, Y in dl[dataset]:\n",
    "            preds = model(X)\n",
    "            Y_preds.append(preds.argmax(dim=-1))\n",
    "            Y_shuffled.append(Y)\n",
    "\n",
    "        Y_shuffled = torch.cat(Y_shuffled)\n",
    "        Y_preds    = torch.cat(Y_preds)\n",
    "\n",
    "        return Y_shuffled, Y_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "setup_seed(42)\n",
    "\n",
    "class LSTM_glove_vecs(torch.nn.Module) :\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, glove_weights) :\n",
    "        super().__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.embeddings.weight.data.copy_(torch.from_numpy(glove_weights))\n",
    "        self.embeddings.weight.requires_grad = False ## freeze embeddings\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_dim, 2)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.embeddings(x)\n",
    "        x = self.dropout(x)\n",
    "        lstm_out, (ht, ct) = self.lstm(x)\n",
    "        return self.linear(ht[-1])\n",
    "    \n",
    "lstm_model = LSTM_glove_vecs(vocab_size, 100, 100, pretrained_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 23/23 [00:07<00:00,  2.94it/s]\n",
      "100%|███████████████████████████████████████████| 23/23 [00:07<00:00,  2.94it/s]\n",
      "100%|███████████████████████████████████████████| 23/23 [00:07<00:00,  2.98it/s]\n",
      "100%|███████████████████████████████████████████| 23/23 [00:07<00:00,  2.95it/s]\n",
      "100%|███████████████████████████████████████████| 23/23 [00:07<00:00,  2.95it/s]\n",
      "100%|███████████████████████████████████████████| 23/23 [00:07<00:00,  2.97it/s]\n",
      "100%|███████████████████████████████████████████| 23/23 [00:07<00:00,  2.93it/s]\n",
      "100%|███████████████████████████████████████████| 23/23 [00:07<00:00,  2.99it/s]\n",
      "100%|███████████████████████████████████████████| 23/23 [00:07<00:00,  2.97it/s]\n",
      "100%|███████████████████████████████████████████| 23/23 [00:07<00:00,  2.97it/s]\n"
     ]
    }
   ],
   "source": [
    "optimizer = Adam(lstm_model.parameters(), lr=learning_rate)\n",
    "TrainingLoop(lstm_model, loss_fn, optimizer, dl['twitter-training-data.txt'], dl['twitter-dev-data.txt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "twitter-dev-data.txt (LSTM): 0.530\n"
     ]
    }
   ],
   "source": [
    "classifier = 'LSTM'\n",
    "pred_list = Prediction(lstm_model, loss_fn, 'twitter-dev-data.txt')[1].tolist()\n",
    "for i in range(len(pred_list)):\n",
    "    if pred_list[i] == 0:\n",
    "        pred_list[i] ='positive'\n",
    "    else:\n",
    "        pred_list[i] ='negative'\n",
    "        \n",
    "id_preds = dict(zip(tweetids['twitter-dev-data.txt'], pred_list))\n",
    "evaluate(id_preds, 'twitter-dev-data.txt', classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bi_LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "setup_seed(42)\n",
    "\n",
    "# basic lstm with glove embedding\n",
    "class Bi_LSTM(torch.nn.Module) :\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, glove_weights) :\n",
    "        super().__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.embeddings.weight.data.copy_(torch.from_numpy(glove_weights))\n",
    "        self.embeddings.weight.requires_grad = False ## freeze embeddings\n",
    "        self.lstm1 = nn.LSTM(embedding_dim, hidden_dim, batch_first=True,bidirectional = True)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.fc1 = nn.Linear(hidden_dim, 100)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.norm1 = nn.BatchNorm1d(100)\n",
    "        \n",
    "        self.fc2 = nn.Linear(100,50)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.norm2 = nn.BatchNorm1d(50)\n",
    "        self.fc3 = nn.Linear(50,25)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.norm3 = nn.BatchNorm1d(25)\n",
    "        self.fc4 = nn.Linear(25,2)\n",
    "        self.sig = nn.Sigmoid() # reduces performance\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.embeddings(x)\n",
    "        x = self.dropout(x)\n",
    "        lstm1_out, (ht1, ct1) = self.lstm1(x)\n",
    "        out = self.dropout(ht1[-1])\n",
    "        out = self.fc1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.norm1(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.norm2(out)\n",
    "        out = self.fc3(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.norm3(out)\n",
    "        out = self.fc4(out)\n",
    "        out = self.sig(out)\n",
    "        return out\n",
    "        \n",
    "    \n",
    "bi_lstm_model = Bi_LSTM(vocab_size, 100, 300, pretrained_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 23/23 [01:10<00:00,  3.07s/it]\n",
      "100%|███████████████████████████████████████████| 23/23 [01:09<00:00,  3.04s/it]\n",
      "100%|███████████████████████████████████████████| 23/23 [01:09<00:00,  3.02s/it]\n",
      "100%|███████████████████████████████████████████| 23/23 [01:08<00:00,  3.00s/it]\n",
      "100%|███████████████████████████████████████████| 23/23 [01:08<00:00,  2.99s/it]\n",
      "100%|███████████████████████████████████████████| 23/23 [01:09<00:00,  3.01s/it]\n",
      "100%|███████████████████████████████████████████| 23/23 [01:09<00:00,  3.00s/it]\n",
      "100%|███████████████████████████████████████████| 23/23 [01:09<00:00,  3.00s/it]\n",
      "100%|███████████████████████████████████████████| 23/23 [01:08<00:00,  2.99s/it]\n",
      "100%|███████████████████████████████████████████| 23/23 [01:08<00:00,  3.00s/it]\n"
     ]
    }
   ],
   "source": [
    "optimizer = Adam(bi_lstm_model.parameters(), lr=learning_rate)\n",
    "TrainingLoop(bi_lstm_model, loss_fn, optimizer, dl['twitter-training-data.txt'], dl['twitter-dev-data.txt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "twitter-dev-data.txt (Bi_LSTM): 0.507\n"
     ]
    }
   ],
   "source": [
    "classifier = 'Bi_LSTM'\n",
    "pred_list = Prediction(bi_lstm_model, loss_fn, 'twitter-dev-data.txt')[1].tolist()\n",
    "for i in range(len(pred_list)):\n",
    "    if pred_list[i] == 0:\n",
    "        pred_list[i] ='positive'\n",
    "    else:\n",
    "        pred_list[i] ='negative'\n",
    "        \n",
    "id_preds = dict(zip(tweetids['twitter-dev-data.txt'], pred_list))\n",
    "evaluate(id_preds, 'twitter-dev-data.txt', classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bi_LSTM_GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "setup_seed(42)\n",
    "\n",
    "class Bi_LSTM_GRU(torch.nn.Module) :\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, glove_weights) :\n",
    "        super().__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.embeddings.weight.data.copy_(torch.from_numpy(glove_weights))\n",
    "        self.embeddings.weight.requires_grad = False ## freeze embeddings\n",
    "        self.lstm1 = nn.LSTM(embedding_dim, hidden_dim, batch_first=True,bidirectional = True)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.fc1 = nn.Linear(hidden_dim, 100)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.norm1 = nn.BatchNorm1d(100)\n",
    "        self.gru = nn.GRU(100,75,batch_first=True)\n",
    "        self.fc2 = nn.Linear(75,50)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.norm2 = nn.BatchNorm1d(50)\n",
    "        self.fc3 = nn.Linear(50,25)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.norm3 = nn.BatchNorm1d(25)\n",
    "        self.fc4 = nn.Linear(25,2)\n",
    "        self.sig = nn.Sigmoid() # reduces performance\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.embeddings(x)\n",
    "        x = self.dropout(x)\n",
    "        lstm1_out, (ht1, ct1) = self.lstm1(x)\n",
    "        out = self.fc1(ht1[-1])\n",
    "        out = self.relu(out)\n",
    "        out = self.norm1(out)\n",
    "        out,_ = self.gru(out) #\n",
    "        out = out.reshape(out.shape[0], -1) #\n",
    "        out = self.fc2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.norm2(out)\n",
    "        out = self.fc3(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.norm3(out)\n",
    "        out = self.fc4(out)\n",
    "        out = self.sig(out)\n",
    "        return out\n",
    "bi_lstm_gru_model = Bi_LSTM_GRU(vocab_size, 100, 300, pretrained_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 23/23 [01:14<00:00,  3.24s/it]\n",
      "100%|███████████████████████████████████████████| 23/23 [01:14<00:00,  3.24s/it]\n",
      "100%|███████████████████████████████████████████| 23/23 [01:14<00:00,  3.24s/it]\n",
      "100%|███████████████████████████████████████████| 23/23 [01:14<00:00,  3.24s/it]\n",
      "100%|███████████████████████████████████████████| 23/23 [01:14<00:00,  3.24s/it]\n",
      "100%|███████████████████████████████████████████| 23/23 [01:14<00:00,  3.25s/it]\n",
      "100%|███████████████████████████████████████████| 23/23 [01:14<00:00,  3.25s/it]\n",
      "100%|███████████████████████████████████████████| 23/23 [01:14<00:00,  3.24s/it]\n",
      "100%|███████████████████████████████████████████| 23/23 [06:18<00:00, 16.45s/it]\n",
      "100%|███████████████████████████████████████████| 23/23 [01:14<00:00,  3.24s/it]\n"
     ]
    }
   ],
   "source": [
    "optimizer = Adam(bi_lstm_gru_model.parameters(), lr=learning_rate)\n",
    "TrainingLoop(bi_lstm_gru_model, loss_fn, optimizer, dl['twitter-training-data.txt'], dl['twitter-dev-data.txt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "twitter-dev-data.txt (Bi_LSTM_GRU): 0.520\n"
     ]
    }
   ],
   "source": [
    "classifier = 'Bi_LSTM_GRU'\n",
    "pred_list = Prediction(bi_lstm_gru_model, loss_fn, 'twitter-dev-data.txt')[1].tolist()\n",
    "for i in range(len(pred_list)):\n",
    "    if pred_list[i] == 0:\n",
    "        pred_list[i] ='positive'\n",
    "    else:\n",
    "        pred_list[i] ='negative'\n",
    "        \n",
    "id_preds = dict(zip(tweetids['twitter-dev-data.txt'], pred_list))\n",
    "evaluate(id_preds, 'twitter-dev-data.txt', classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build sentiment classifiers\n",
    "You need to create your own classifiers (at least 3 classifiers). For each classifier, you can choose between the bag-of-word features and the word-embedding-based features. Each classifier has to be evaluated over 3 test sets. Make sure your classifier produce consistent performance across the test sets. Marking will be based on the performance over all 5 test sets (2 of them are not provided to you)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "twitter-dev-data.txt (LSTM): 0.327\n"
     ]
    }
   ],
   "source": [
    "classifier = 'LSTM'\n",
    "model = bi_lstm_gru_model\n",
    "testset = 'twitter-dev-data.txt'\n",
    "pred_list = Prediction(model, loss_fn, testset)[1].tolist()\n",
    "for i in range(len(pred_list)):\n",
    "    if pred_list[i] == 0:\n",
    "        pred_list[i] ='positive'\n",
    "    else:\n",
    "        pred_list[i] ='negative'\n",
    "        \n",
    "        \n",
    "id_preds = dict(zip(tweetids[testset], pred_list))\n",
    "evaluate(id_preds, testset, classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Logistic Regression\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aryamathew/opt/anaconda3/envs/tensorflow/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "semeval-tweets/twitter-test1.txt (bow-Logistic Regression): 0.467\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aryamathew/opt/anaconda3/envs/tensorflow/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "semeval-tweets/twitter-test2.txt (bow-Logistic Regression): 0.495\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aryamathew/opt/anaconda3/envs/tensorflow/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "semeval-tweets/twitter-test3.txt (bow-Logistic Regression): 0.456\n",
      "Training Decision Tree\n",
      "semeval-tweets/twitter-test1.txt (bow-Decision Tree): 0.261\n",
      "semeval-tweets/twitter-test2.txt (bow-Decision Tree): 0.275\n",
      "semeval-tweets/twitter-test3.txt (bow-Decision Tree): 0.255\n",
      "semeval-tweets/twitter-test1.txt (bow-Naive Bayes): 0.487\n",
      "semeval-tweets/twitter-test2.txt (bow-Naive Bayes): 0.509\n",
      "semeval-tweets/twitter-test3.txt (bow-Naive Bayes): 0.484\n",
      "Training LSTM\n",
      "semeval-tweets/twitter-test1.txt (bow-LSTM): 0.492\n",
      "semeval-tweets/twitter-test2.txt (bow-LSTM): 0.544\n",
      "semeval-tweets/twitter-test3.txt (bow-LSTM): 0.490\n",
      "Training Bi_LSTM\n",
      "semeval-tweets/twitter-test1.txt (bow-Bi_LSTM): 0.544\n",
      "semeval-tweets/twitter-test2.txt (bow-Bi_LSTM): 0.566\n",
      "semeval-tweets/twitter-test3.txt (bow-Bi_LSTM): 0.522\n",
      "Training Bi_LSTM_GRU\n",
      "semeval-tweets/twitter-test1.txt (bow-Bi_LSTM_GRU): 0.543\n",
      "semeval-tweets/twitter-test2.txt (bow-Bi_LSTM_GRU): 0.551\n",
      "semeval-tweets/twitter-test3.txt (bow-Bi_LSTM_GRU): 0.503\n"
     ]
    }
   ],
   "source": [
    "# Buid traditional sentiment classifiers. An example classifier name 'svm' is given\n",
    "# in the code below. You should replace the other two classifier names\n",
    "# with your own choices. For features used for classifier training, \n",
    "# the 'bow' feature is given in the code. But you could also explore the \n",
    "# use of other features.\n",
    "for classifier in ['Logistic Regression','Decision Tree','Naive Bayes','LSTM','Bi_LSTM','Bi_LSTM_GRU']:\n",
    "    for features in ['bow']:\n",
    "        # Skeleton: Creation and training of the classifiers\n",
    "        #if classifier == 'svm':\n",
    "            # write the svm classifier here\n",
    "            #model = svm.SVC(kernel='linear', C=1)\n",
    "            #print('Training ' + classifier)\n",
    "        if classifier == 'Logistic Regression':\n",
    "            # write the classifier 2 here\n",
    "            model = LogisticRegression()\n",
    "            print('Training ' + classifier)\n",
    "        elif classifier == 'Random Forest':\n",
    "            # write the classifier 3 here\n",
    "            model = RandomForestClassifier(n_estimators=400, random_state=11)\n",
    "            print('Training ' + classifier)\n",
    "        elif classifier == 'Decision Tree':\n",
    "            model = DecisionTreeClassifier(max_depth=10, random_state=0)\n",
    "            print('Training ' + classifier)\n",
    "        elif classifier == 'Naive Bayes':\n",
    "            model = GaussianNB()\n",
    "        elif classifier == 'LSTM':\n",
    "            # write the LSTM classifier here\n",
    "            class LSTM_glove_vecs(torch.nn.Module) :\n",
    "                def __init__(self, vocab_size, embedding_dim, hidden_dim, glove_weights) :\n",
    "                    super().__init__()\n",
    "                    self.embeddings = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "                    self.embeddings.weight.data.copy_(torch.from_numpy(glove_weights))\n",
    "                    self.embeddings.weight.requires_grad = False ## freeze embeddings\n",
    "                    self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "                    self.linear = nn.Linear(hidden_dim, 2)\n",
    "                    self.dropout = nn.Dropout(0.2)\n",
    "        \n",
    "                def forward(self, x):\n",
    "                    x = self.embeddings(x)\n",
    "                    x = self.dropout(x)\n",
    "                    lstm_out, (ht, ct) = self.lstm(x)\n",
    "                    return self.linear(ht[-1])\n",
    "            # model = LSTM_glove_vecs(vocab_size, 100, 100, pretrained_weights)\n",
    "            # learning_rate = 1e-3\n",
    "            # loss_fn = nn.CrossEntropyLoss() # Loss Function\n",
    "            #optimizer = optim.SGD(text_classifier.parameters(), lr=0.01, momentum=0.9)\n",
    "            # optimizer = Adam(model.parameters(), lr=learning_rate) # Optimizer\n",
    "            # TrainingLoop(model, loss_fn, optimizer, dl['twitter-training-data.txt'], dl['twitter-dev-data.txt'])\n",
    "            print('Training ' + classifier)\n",
    "        elif classifier == 'Bi_LSTM':\n",
    "            class Bi_LSTM(torch.nn.Module) :\n",
    "                def __init__(self, vocab_size, embedding_dim, hidden_dim, glove_weights) :\n",
    "                    super().__init__()\n",
    "                    self.embeddings = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "                    self.embeddings.weight.data.copy_(torch.from_numpy(glove_weights))\n",
    "                    self.embeddings.weight.requires_grad = False ## freeze embeddings\n",
    "                    self.lstm1 = nn.LSTM(embedding_dim, hidden_dim, batch_first=True,bidirectional = True)\n",
    "                    self.dropout = nn.Dropout(0.2)\n",
    "                    self.fc1 = nn.Linear(hidden_dim, 100)\n",
    "                    self.relu = nn.ReLU()\n",
    "                    self.norm1 = nn.BatchNorm1d(100)\n",
    "        \n",
    "                    self.fc2 = nn.Linear(100,50)\n",
    "                    self.relu = nn.ReLU()\n",
    "                    self.norm2 = nn.BatchNorm1d(50)\n",
    "                    self.fc3 = nn.Linear(50,25)\n",
    "                    self.relu = nn.ReLU()\n",
    "                    self.norm3 = nn.BatchNorm1d(25)\n",
    "                    self.fc4 = nn.Linear(25,2)\n",
    "                    self.sig = nn.Sigmoid() # reduces performance\n",
    "        \n",
    "                def forward(self, x):\n",
    "                    x = self.embeddings(x)\n",
    "                    x = self.dropout(x)\n",
    "                    lstm1_out, (ht1, ct1) = self.lstm1(x)\n",
    "                    out = self.dropout(ht1[-1])\n",
    "                    out = self.fc1(out)\n",
    "                    out = self.relu(out)\n",
    "                    out = self.norm1(out)\n",
    "                    out = self.fc2(out)\n",
    "                    out = self.relu(out)\n",
    "                    out = self.norm2(out)\n",
    "                    out = self.fc3(out)\n",
    "                    out = self.relu(out)\n",
    "                    out = self.norm3(out)\n",
    "                    out = self.fc4(out)\n",
    "                    out = self.sig(out)\n",
    "                    return out\n",
    "            print('Training ' + classifier)\n",
    "        elif classifier == 'Bi_LSTM_GRU':\n",
    "            class Bi_LSTM_GRU(torch.nn.Module) :\n",
    "                def __init__(self, vocab_size, embedding_dim, hidden_dim, glove_weights) :\n",
    "                    super().__init__()\n",
    "                    self.embeddings = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "                    self.embeddings.weight.data.copy_(torch.from_numpy(glove_weights))\n",
    "                    self.embeddings.weight.requires_grad = False ## freeze embeddings\n",
    "                    self.lstm1 = nn.LSTM(embedding_dim, hidden_dim, batch_first=True,bidirectional = True)\n",
    "                    self.dropout = nn.Dropout(0.2)\n",
    "                    self.fc1 = nn.Linear(hidden_dim, 100)\n",
    "                    self.relu = nn.ReLU()\n",
    "                    self.norm1 = nn.BatchNorm1d(100)\n",
    "                    self.gru = nn.GRU(100,75,batch_first=True)\n",
    "                    self.fc2 = nn.Linear(75,50)\n",
    "                    self.relu = nn.ReLU()\n",
    "                    self.norm2 = nn.BatchNorm1d(50)\n",
    "                    self.fc3 = nn.Linear(50,25)\n",
    "                    self.relu = nn.ReLU()\n",
    "                    self.norm3 = nn.BatchNorm1d(25)\n",
    "                    self.fc4 = nn.Linear(25,2)\n",
    "                    self.sig = nn.Sigmoid() # reduces performance\n",
    "        \n",
    "                def forward(self, x):\n",
    "                    x = self.embeddings(x)\n",
    "                    x = self.dropout(x)\n",
    "                    lstm1_out, (ht1, ct1) = self.lstm1(x)\n",
    "                    out = self.fc1(ht1[-1])\n",
    "                    out = self.relu(out)\n",
    "                    out = self.norm1(out)\n",
    "                    out,_ = self.gru(out) #\n",
    "                    out = out.reshape(out.shape[0], -1) #\n",
    "                    out = self.fc2(out)\n",
    "                    out = self.relu(out)\n",
    "                    out = self.norm2(out)\n",
    "                    out = self.fc3(out)\n",
    "                    out = self.relu(out)\n",
    "                    out = self.norm3(out)\n",
    "                    out = self.fc4(out)\n",
    "                    out = self.sig(out)\n",
    "                    return out\n",
    "            print('Training ' + classifier)\n",
    "        else:\n",
    "            print('Unknown classifier name' + classifier)\n",
    "            continue\n",
    "\n",
    "        # Predition performance of the classifiers\n",
    "        for testset in testsets:\n",
    "            id_preds = {}\n",
    "            # write the prediction and evaluation code here\n",
    "            if classifier == 'LSTM':\n",
    "                pred_list = Prediction(lstm_model, loss_fn, testset)[1].tolist()\n",
    "                for i in range(len(pred_list)):\n",
    "                    if pred_list[i] == 0:\n",
    "                        pred_list[i] ='positive'\n",
    "                    else:\n",
    "                        pred_list[i] ='negative'\n",
    "            elif classifier == 'Bi_LSTM':\n",
    "                pred_list = Prediction(bi_lstm_model, loss_fn, testset)[1].tolist()\n",
    "                for i in range(len(pred_list)):\n",
    "                    if pred_list[i] == 0:\n",
    "                        pred_list[i] ='positive'\n",
    "                    else:\n",
    "                        pred_list[i] ='negative'\n",
    "            elif classifier == 'Bi_LSTM_GRU':\n",
    "                pred_list = Prediction(bi_lstm_gru_model, loss_fn, testset)[1].tolist()\n",
    "                for i in range(len(pred_list)):\n",
    "                    if pred_list[i] == 0:\n",
    "                        pred_list[i] ='positive'\n",
    "                    else:\n",
    "                        pred_list[i] ='negative'\n",
    "                \n",
    "            else:\n",
    "                document = []\n",
    "                for dataset in ['twitter-training-data.txt', testset] :\n",
    "                    document += tweets_preprocessed[dataset]\n",
    "                # TF-IDF feature matrix\n",
    "                tfidf = tfidf_vectorizer.fit_transform(document)\n",
    "                ytrain = tweetgts['twitter-training-data.txt']\n",
    "                ytest = tweetgts[testset]\n",
    "                train_tfidf = tfidf[:45101,:]\n",
    "                test_tfidf = tfidf[45101:,:]\n",
    "\n",
    "                xtrain_tfidf = train_tfidf[range(len(ytrain))]\n",
    "                xtest_tfidf = test_tfidf[range(len(ytest))]\n",
    "                if classifier=='Naive Bayes':\n",
    "                    model.fit(xtrain_tfidf.toarray(), ytrain)\n",
    "                    preds = model.predict(xtest_tfidf.toarray())\n",
    "                else:\n",
    "                    model.fit(xtrain_tfidf, ytrain)\n",
    "                    preds = model.predict(xtest_tfidf)\n",
    "                pred_list = list(preds)\n",
    "            \n",
    "            \n",
    "            id_preds = dict(zip(tweetids[testset], pred_list))\n",
    "\n",
    "            testset_name = testset\n",
    "            testset_path = join('semeval-tweets', testset_name)\n",
    "            evaluate(id_preds, testset_path, features + '-' + classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bi_LSTM_GRU\n",
      "twitter-test3.txt\n",
      "            positive  negative  neutral\n",
      "positive    0.464     0.113     0.423     \n",
      "negative    0.227     0.427     0.347     \n",
      "neutral     0.000     0.000     0.000     \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classifier)\n",
    "print(testset)\n",
    "confusion(id_preds, testset, classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "testset = 'twitter-dev-data.txt'\n",
    "prediction_list = list(np.zeros(len(tweetids[testset])))\n",
    "for i in range(len(prediction_list)):\n",
    "    prediction_list[i]=random.randint(0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(prediction_list)):\n",
    "    if prediction_list[i] == 0:\n",
    "        prediction_list[i] ='positive'\n",
    "    else:\n",
    "        prediction_list[i] ='negative'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "twitter-dev-data.txt (Random Classifier): 0.376\n"
     ]
    }
   ],
   "source": [
    "id_preds = dict(zip(tweetids[testset], prediction_list))\n",
    "evaluate(id_preds, testset, 'Random Classifier')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
